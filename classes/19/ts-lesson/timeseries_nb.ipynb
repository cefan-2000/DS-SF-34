{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Assembly - Time Series & Auto Regressive (AR) Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series: \n",
    "\n",
    "Any data in which the data points change over time. \n",
    " \n",
    "A time series data set is common in buisiness data like seasonal trends and stocks. Also in natural events, like climate and disasters. A time series analysis can be used to study the changes of a feature over time - how does this week's sales affect next week's? \n",
    "\n",
    "Typically we want to separate time series data into three components: \n",
    "\n",
    "1) Trends: general increses or decreases over time. \n",
    "\n",
    "2) Seasonality: regularly repeating changes over time. \n",
    "\n",
    "3) Cycles: aperiodic - changes occur but with no fixed time interval. \n",
    "\n",
    "Here are some examples: \n",
    "\n",
    "**Fireworks Injury: downward trend, no seasonality**\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/momonala/DS_tutorials/master/files/fireworks_injury_trend.png' width='350'>\n",
    "\n",
    "**Holiday Sales: upward trend, seasonality**\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/momonala/DS_tutorials/master/files/trends_with_season.png' width='350'>\n",
    "\n",
    "**Goolgle searches for 'iPhone': seasonal, no trend**\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/momonala/DS_tutorials/master/files/iphone_trend.png' width='350'>\n",
    "\n",
    "**Stock Price: aperiodic**\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/momonala/DS_tutorials/master/files/kellog_stock_aperiodic.png' width='350'>\n",
    "\n",
    "One relevant data set to play with is [Google Trends](https://trends.google.com/trends/). Here you can explore frequency and location of search terms over time. Data scientist Seth Stephens-Davidowitz has written extensively on this topic as a columnnist [The New York Times](https://www.nytimes.com/by/seth-stephens-davidowitz). Take a few minutes on Google Trends and see if you can find search terms that exhibit the 5 types of behavior above (4 permutations of trends/seasons + aperiodic). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving Averages \n",
    "\n",
    "Often, the data we are working with is noisy and has many high frequency random fluctuations in the lower frequency signal. Once way to smooth out these fluctuations and outliers is to apply a moving average. This will help us visualize trends. \n",
    "\n",
    "A moving average replaces each data point with an average of $k$ data points behind it. $k$ is referred to as the window size, and the average can either be the mean or median. \n",
    "\n",
    "$$\n",
    "SMA_{value} = \\frac{1}{k} \\sum\\limits_{i=1}^k V_i\n",
    "$$\n",
    "\n",
    "where $V_i$ is the vector of values to be averaged. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "File ../data/tsla_stock.csv does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-51e117c4afae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtsla\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/tsla_stock.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtsla\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtsla\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#set date as index on df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtsla\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtsla\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/seanmcghee/anaconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    644\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/seanmcghee/anaconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/seanmcghee/anaconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/seanmcghee/anaconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    921\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 923\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    924\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/seanmcghee/anaconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1388\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1390\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1392\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas/parser.c:4184)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._setup_parser_source (pandas/parser.c:8449)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: File ../data/tsla_stock.csv does not exist"
     ]
    }
   ],
   "source": [
    "tsla = pd.read_csv('../data/tsla_stock.csv') \n",
    "tsla.Date = pd.to_datetime(tsla['Date']) #set date as index on df \n",
    "tsla.set_index('Date', inplace=True)\n",
    "tsla.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#calculate moving average \n",
    "moving_avg = tsla.rolling(window=40).mean() \n",
    "\n",
    "#plot \n",
    "fig = plt.figure(figsize=(14, 5))\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "ax1.plot(tsla)\n",
    "ax1.plot(moving_avg, c='r')\n",
    "plt.title('Tesla Stock'); plt.ylabel('price'); plt.xlabel('date')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we were able to smooth noise in the stock price using a 40 day moving average. I encourage you to tweak the window size and observe the effects. Note that prices after March 2017 are not calculated because we don't have enough datapoints in our window. You can read more about work arounds in the docstring. \n",
    "\n",
    "One clear flaw we can see is a lag in the moving average vs. the real price. This is because a value at any time will be more correlated to values that are more recent to it in the past. The price is more likely to be like yesterday's price, rather than last months. The solution to this issue is to weight the moving averages more at more recent time points. \n",
    "\n",
    "A weighted moving average applies weights to the averaging function, typically so that more recent values have a larger weight on the new value. \n",
    "$$\n",
    "WMA_{value} = \\frac{1}{k} \\sum\\limits_{i=1}^k W_i V_i\n",
    "$$\n",
    "\n",
    "where $W_i$ is the weights vector, and $V_i$ is you values vector. \n",
    "\n",
    "Unfortunately, there is no pandas function for weighted moving averages at the time of writing. But it is possible to code a version with numpy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def linear_weights(n):\n",
    "    #linear spaced weights \n",
    "    weights = np.linspace(0, n-1, n)\n",
    "    return weights \n",
    "\n",
    "def weighted_ma(vec, k): #input series, window-size \n",
    "    #a function to calcualte weighted moving average\n",
    "    vec = vec[::-1] #sort stock data by most recent first \n",
    "    weights = linear_weights(k) #apply weighted moving average\n",
    "    wma = np.zeros(shape=(len(vec)))\n",
    "    for i, _ in enumerate(vec): #convolve over input series \n",
    "        if i == 0: #prevent operand error  \n",
    "            window = vec[(-k-i):]\n",
    "        else: \n",
    "            window = vec[(-k-i):-i]\n",
    "        if len(window) < k: #no padding\n",
    "            break\n",
    "        fx = (sum(window*weights)) / sum(weights) #compute weighted average\n",
    "        wma[i] = fx #append to new array \n",
    "    return wma "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wma = weighted_ma(tsla['tsla_adj'], 40) #implement functions \n",
    "wma = pd.DataFrame(data=wma, index=tsla.index)\n",
    "\n",
    "fig = plt.figure(figsize=(14, 5))\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "ax1.plot(tsla)\n",
    "ax1.plot(moving_avg, c='r')\n",
    "ax1.plot(wma, c='g')#plot weighted moving average in green \n",
    "plt.title('Tesla Stock'); plt.ylabel('price'); plt.xlabel('date');b_patch = mpatches.Patch(color='b', label='True data');r_patch = mpatches.Patch(color='r', label='Simple MA');g_patch = mpatches.Patch(color='g', label='Weighted MA');plt.legend(handles=[b_patch, r_patch, g_patch])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that the green curve, the weighted moving average, does a better job at following the true price while minimizing noise. We can change the way the weights are calculated to be linear, exponential, etc. to change the output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autocorrelation\n",
    "\n",
    "Previsouly, we were curious about how two variables are correlted (height & weight, education & salary). In time series, this is equally important, but we use a new measure called autocorrelation. \n",
    "\n",
    "Autocorrelation is the measure of how correlated a variable is with itself at a previous time. It is the same as calculating the correation between two different series, but instead we are using the same series twice, one in its original form, the other at a different point in time. We use the term **lag** to define this time shift. A lag of 1 calulates the correlation of a value with the preivous one, a lag of 10 calcualtes correlation 10 values back. \n",
    "\n",
    "Typically, a high quality time model will require some autocorrelation in the data. Autocorrelation is useful because we can change the lag term to figure out how far back in time our averages apply, and if/what the periodicty of our model is.\n",
    "\n",
    "Below is the python implementation of autocorrelation. You first select the time series you want to operate on, then call \"resample\" and select the time frame (days in our case), calulcate the mean, and then the autocorrelation which takes in the lag size as an argument. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tsla['tsla_adj'].resample('D').mean().autocorr(lag=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interpreation of this result is the same as the .corr() method. A value of 0 means no correlation, and value of 1 or -1 means full of fully inverse correlation. Try changing the above lag value think about the results.\n",
    "\n",
    "We can also plot many lags at once, both with statsmodels and pandas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "plot_acf(tsla['tsla_adj'], lags=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.tools.plotting.autocorrelation_plot(tsla['tsla_adj']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way to interpret this chart is that  number of lags is on the x-axis and correlation is on the y; we are interested in the blue dots at the top. The fewer lags seem to correlate more with future values. This isn't that exciting for stock data, but inputting sales or climate data could give insight to monthly or yearly cyclic trends. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation - workaround\n",
    "\n",
    "Unlike previous predictive models, we nowuse previous time outcomes as inputs for prediction. We will also not be able to use standard cross validation, since the relative location of data points are importnat. Instead, we will need to split our dataset temporaly. \n",
    "\n",
    "<img src='https://raw.githubusercontent.com/momonala/DS_tutorials/master/files/time_series_cross_val.png' width='500'>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = len(tsla['tsla_adj']) #get # of days \n",
    "\n",
    "#split data set by 75% and 25% sections \n",
    "train = tsla['tsla_adj'][:int(.75*n)]\n",
    "test = tsla['tsla_adj'][int(.75*n):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decomposition\n",
    "\n",
    "Decomposition is the process of removing trends or seasonal periodicity from a time series sequence. This allows us to tease out specific features in the data, as well as normalize the data so that it will satisfy the assumptions we require for various quantitative and statistical modelling. \n",
    "\n",
    "## Detrending\n",
    "\n",
    "Most time series statistical modeling assumes stationarity - that the means and variance of values stay constant. We may have sales increases or decreases, but the average sales stays the same. This may not always hold true in the real world, so we must be aware of this bias, and know when to control it.\n",
    "\n",
    "When this stationarity assumption doens't hold true, we can alter our data so that our analysis can still be applied. Detrendng removes major trends from the data. Typically, we just fit a line to the trend and make a new series that is the difference between the true series and the trend. \n",
    "\n",
    "Below I removed fit the tesla stock to a linear model, then subtracted out the values of the best fit line from the true tesla price stock. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as sm\n",
    "\n",
    "y = tsla['tsla_adj'] #dependent variable \n",
    "x = np.linspace(0, len(y), len(y)) #arbitrary sequence of integers to replace dates  \n",
    "result = sm.ols(formula=  \"y~x\" , data=tsla).fit()\n",
    "y_vals = (result.params[1]*x) + result.params[0] #y=mx+b\n",
    "y_vals_plot = pd.DataFrame(data=y_vals, index=tsla.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tsla_detrend = tsla['tsla_adj']-y_vals #subtract best fit from true price \n",
    "\n",
    "fig = plt.figure(figsize=(14, 5))\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "ax1.plot(tsla)\n",
    "ax1.plot(y_vals_plot, c='c')\n",
    "ax1.plot(tsla_detrend, c='r') #plot the detrended data in red \n",
    "ax1.plot(y*0, c='g') #green line through 0 \n",
    "plt.title('Tesla Stock'); plt.ylabel('price'); plt.xlabel('date');b_patch = mpatches.Patch(color='b', label='True data');c_patch = mpatches.Patch(color='c', label='Best fit for true data');r_patch = mpatches.Patch(color='r', label='Detrended data');g_patch = mpatches.Patch(color='g', label='Zero');plt.legend(handles=[b_patch, c_patch, r_patch, g_patch])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the mean is now (roughly) centered on 0, implying stationarity. \n",
    "\n",
    "## Seasonality Decomposition \n",
    " \n",
    "The other main type of decomposition is seasonal decomposition, which uses moving averages to remove seasonal periodicty from data. We will use sales from a buisiness to explore this. \n",
    "\n",
    "#### Dataset: \n",
    "\n",
    "Rossmann operates over 3,000 drug stores in 7 European countries. Currently, Rossmann store managers are tasked with predicting their daily sales for up to six weeks in advance. Store sales are influenced by many factors, including promotions, competition, school and state holidays, seasonality, and locality. With thousands of individual managers predicting sales based on their unique circumstances, the accuracy of results can be quite varied.\n",
    "\n",
    "    Id - an Id that represents a (Store, Date) duple within the test set\n",
    "    Store - a unique Id for each store\n",
    "    Sales - the turnover for any given day (this is what you are predicting)\n",
    "    Customers - the number of customers on a given day\n",
    "    Open - an indicator for whether the store was open: 0 = closed, 1 = open\n",
    "    StateHoliday - Normally all stores, with few exceptions, are closed on state holidays\n",
    "    SchoolHoliday - indicates if the (Store, Date) was affected by the closure of public schools\n",
    "    Promo - indicates whether a store is running a promo on that day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_page = 'https://raw.githubusercontent.com/momonala/ds-sf-29/master/lessons/lesson-14/assets/dataset/rossmann.csv'\n",
    "data = pd.read_csv(data_page, low_memory=False)\n",
    "\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.set_index('Date', inplace=True)\n",
    "data['Year'] = data.index.year\n",
    "data['Month'] = data.index.month\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Filter to days store 1 was open\n",
    "store1_data = data[data.Store == 1]\n",
    "store1_open_data = store1_data[store1_data.Open==1]\n",
    "store1_open_data[['Sales']].plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that the sales data is noisy, but clearly is centered on a mean of ~$5000, and has cyclic seasons, peaking with Christmas sales in December. \n",
    "\n",
    "We will now decompose the time series data into its SMA trendline and periodic effects in statsmodels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm  \n",
    "from statsmodels.tsa.seasonal import seasonal_decompose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "decomposition = seasonal_decompose(store1_open_data[['Sales']], freq=12)  # is the # of days of lag for the Trend\n",
    "fig = plt.figure()  \n",
    "fig = decomposition.plot()  \n",
    "fig.set_size_inches(15, 8) #make larger "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This decomposition clearly shows us the peak in holiday sales as well as the seasonal/periodic weekly sales that (most likely) peak on weekends, and lower on Mondays. \n",
    "\n",
    "Lets get quantitative feedback about the weekly sales by checking autocorrelation. Change the lag argument to consider daily, weekly, and monthly time shifts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "store1_data['Sales'].resample('D').mean().autocorr(lag=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()  \n",
    "plot_acf(store1_data[['Sales']], lags=35)\n",
    "fig.set_size_inches(15, 8) #make larger "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weekly correlation is clear here. \n",
    "\n",
    "-----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Time Series Modeling \n",
    "\n",
    "So we understand time series data and how to apply stats to understand historical trends. But history is most useful when we apply its findings to the future. We may want to know the number of sales in a future month, anticipated web traffic, stock predictions etc.. The rest of the lecture will discuss a new set of models to forecast time series data.\n",
    "\n",
    "\n",
    "## Mini Review of Multi-linear Regression\n",
    "\n",
    "$$\n",
    "\\hat y_i= b_0 + b_1x_1 + b_2x_2 + ... + b_nx_n \n",
    "$$\n",
    "\n",
    "#### <i>Why can't we use this format for time series data?</i>\n",
    "\n",
    "\n",
    "Because we don't have predictors! We only previous values of the series. What do we do??\n",
    "\n",
    "### Take 4 minutes to retrofit linear regression for time-series\n",
    "\n",
    "$$\n",
    "\\hat y_t =\n",
    "$$\n",
    "\n",
    "<br> <br> <br> <br> \n",
    "# Autoregressive (AR) Models \n",
    "\n",
    "\n",
    "An autoregressive, or AR$(p)$, model is created by regressing a time series on its past values, its lags. It will try to predict future values based on its passed. This is similar to past regression models we have done, except as input, we take previous time's outcome. \n",
    "\n",
    "In the autoregression, we are learning regression coefficients for each of the previous lag values. Therefore we will have the same number of coefficents as lag values. Going from AR($1$) to AR($2$) can signficiantly increase multicollinearity, since we are adding more coefficients. \n",
    "\n",
    "$$\n",
    "y_t = b_0 + b_1 y_{t - 1} +...+ b_p y_{t-p} + \\epsilon_t\n",
    "$$\n",
    "\n",
    "Where $y_{t - 1}$ represents the value of the time series at time $(t - 1)$, $b$ is a coefficent, and $\\epsilon_t$ is the error term.\n",
    "\n",
    "Intuitively, if we have a time series of sales per week, $x_t$, we can predict $x$ term based on the previous week's sales. This would be an AR($1$) model because the prediction depends on only one lagged value.  We would get the following equation, where $b$ tells us the relationship between the previous value, $Y_{t-1}$m and the predicted value, $Y_t$ \n",
    "\n",
    "$$\n",
    "Y_t = b_0 + b_1 Y_{t-1}\n",
    "$$\n",
    "\n",
    "#### <i>What are the possible long-term outcomes of an AR(1) model? How is this influenced by various beta values?</i>\n",
    "<br><br><br>\n",
    "\n",
    "It will either converge or explode. A value for $|b_1|$ greater than 1 would indicate growth over the previous value, otherwise predictions stabilize at a specific value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasting Walmart Sales with AR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "walmart = pd.read_csv(\"walmart-weekly-sales.csv\")\n",
    "walmart.Date = pd.to_datetime(walmart.Date)\n",
    "walmart = walmart.set_index(\"Date\")\n",
    "walmart.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "walmart.Weekly_Sales.plot(figsize = (15, 6), title = \"Walmart Weekly Sales\")\n",
    "\n",
    "total_count = walmart.shape[0]\n",
    "train_count = int(total_count * 0.75) #calculate vertical line placement\n",
    "\n",
    "plt.axvline('2012-2-24', linewidth = 3, color = \"red\", ls = \"dashed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.ar_model import AR\n",
    "from statsmodels.tsa import arima_model\n",
    "\n",
    "def PlotAR(data, p = 1):\n",
    "    count = data.shape[0]\n",
    "    traincnt = int(count * 0.75)\n",
    "    \n",
    "    traindates = data[:traincnt].index\n",
    "    trainvals = data[:traincnt].values\n",
    "    testdates = data[traincnt:].index\n",
    "    testvals = data[traincnt:].values\n",
    "    \n",
    "    #train AR(p) model\n",
    "    ar = AR(trainvals, dates = traindates).fit(maxlag = p)\n",
    "    \n",
    "    #forecast on test set\n",
    "    test_start = testdates[0]\n",
    "    test_end = testdates[-1]\n",
    "    predictions = ar.predict(start = test_start, end = test_end)\n",
    "    \n",
    "\n",
    "    #plot true vs predicted\n",
    "    plt.plot(testvals, label = \"Truth\")\n",
    "    plt.plot(predictions, label = \"Predicted\", color = \"red\")\n",
    "    \n",
    "    plt.legend(loc = \"best\")\n",
    "    plt.title(\"AR({}) model: Predictions vs Test\".format(p))\n",
    "    plt.show\n",
    "\n",
    "PlotAR(walmart, p = 15)   # p = weeks in past to look for seasonality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see the weights of the coefficients for the AR model. Note the high weight on day 7 and 14. What contributes to this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moving Average (MA) Models\n",
    "\n",
    "As opposed to AR models, MA models do not take previous outputs into account. Instead, they attemp to predict the next value based on previous prediction error. \n",
    "\n",
    "This is useful for handling specific or abrupt changes in a system. MA models incorporate these changes much quicker than an AR model, and are therefore more responsive to change - such as an item going out of stock or a sudden rise in popularity that causes sales. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This type of model is useful for small scale trends, such as an increase in demand or change in a variable that gradually increases sales. \n",
    "\n",
    "$$\n",
    "y_i = \\mu + b_1 \\epsilon_{i - 1} +...+ b_q \\epsilon_{i-q}\n",
    "$$\n",
    "\n",
    "    Note: Autoregressive processes will tend to have more extreme values than data drawn from say a normal distribution. This is because the value at each time point is influenced by recent values. If the series randomly jumps up, it is more likely to stay up than a non-autoregressive series. This is known as 'fat-tailledness' (fat-tailed distribution) because the extremes will be fatter than in a normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARMA: combining strengths of AR and MA\n",
    "\n",
    "\n",
    "AR picks up on long term trends; MA reacts quickly short-term shocks. Why not join their forces?\n",
    "\n",
    "<img src=\"https://photos-3.dropbox.com/t/2/AAAY_BJ-U6_hhWSHUabBVsXjjFSwqfr3qKk3IqZFcvT4PA/12/145750/png/32x32/1/_/1/2/Screenshot%202017-06-20%2012.50.47.png/EIayKhj87YrhBCAHKAc/nkN579pQRlRHVPEnwaVtr5AghJJ1DVzmOS1xmUaeknc?size=2048x1536&size_mode=3\", width=350>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "- Time-series is different than the other data we've touched so far because observations aren't independant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Reading \n",
    "--------------------------------------------------------------\n",
    "\n",
    "* [Quantopian (Quantitative Finance Company) lecture on autocorrelation](https://www.youtube.com/watch?v=fnrSZvla51Y)\n",
    "\n",
    "* [Everybody Lies - Book on uncovering human nature with Google Trends](https://www.amazon.com/Everybody-Lies-Internet-About-Really-ebook/dp/B01AFXZ2F4?SubscriptionId=AKIAILSHYYTFIVPWUY6Q&tag=duckduckgo-ffab-20&linkCode=xm2&camp=2025&creative=165953&creativeASIN=B01AFXZ2F4)\n",
    "\n",
    "* [A nice notebook on Time Series and ARIMA](https://github.com/seanabu/seanabu.github.io/blob/master/Seasonal_ARIMA_model_Portland_transit.ipynb)\n",
    "---------------------------------------------------------------\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
